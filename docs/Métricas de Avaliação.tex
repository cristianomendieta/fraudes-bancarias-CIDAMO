\documentclass{article}

\usepackage[portuguese]{babel}

\usepackage[letterpaper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}

\title{Métricas de Avaliação}
\author{Mateus Fernandes de Souza}

\begin{document}
\maketitle

\section{Definições}


\subsection{Acurácia}
É a razão entre as predições corretas pelo total.
\begin{equation*}
    Acurácia = \frac{TP + TN}{TP + FN + TN + FP}
\end{equation*}

\subsection{Precision}
É o número de verdadeiros positivos, dividido pelo número de positivos estimados pelo modelo
\begin{equation*}
    Precision = \frac{TP}{TP+ FP}
\end{equation*}

\subsection{Recall}
Dado que o estado verdadeiro é positivo, qual a proporção de verdadeiro positivo.
\begin{equation*}
    Recall = \frac{TP}{TP+ FN}
\end{equation*}

\subsection{F1-Score}
É uma média harmônica entre o Precision e o Recall.
\begin{equation*}
    F1-Score = 2*\frac{Precision*Recall}{Precision  + Recall} = \frac{2TP}{2TP+ FP +FN}
\end{equation*}

\subsection{ROC-AUC}
A curva Roc cria uma curva em um gráfico com 2 variáveis(figura 1): no eixo X é a taxa de falsos positivos e no eixo Y é a taxa de verdadeiros positivos. Já o AUC é a área de baixo, quanto mais próximo de 1, melhor será o seu modelo.


\section{Quando Utilizar}


\subsection{Acurácia}
Em geral é uma boa forma de se verificar a qualidade de um modelo, pois é uma métrica que avalia o corpo geral do resultado do seu código, porém ela considera os erros do tipo I e II com o mesmo peso, caso os tipos de erro tenham alguma importância diferente, sendo um mais impactante que o outro, não é o mais viável e recomendável

\subsection{Precision}
É uma boa opção quando o erro do tipo I é mais importante que o erro do tipo II. Define a quantidade que o modelo disse que são positivos, quantos realmente são positivos. Ele dá uma sensação de impacto.

\subsection{Recall}
É uma boa opção quando o erro do tipo II é mais importante que o erro do tipo I. Define a quantade que são positivos, quantos o modelo acertou. Ele dá uma sensação de captura.

\subsection{F1-Score}
É uma boa forma para se verificar os erros em geral do modelo, vendo que ele faz uma média harmônica dos dois resultados anteriorres. Quando os erros são mais importantes que os acertos na previsão, é uma métrica recomendável. OBS: Complementando com essa métrica, temos o $F\beta$ Score, que simplesmente adiciona um peso para o Precision no cálculo gerando assim uma diferença de importância entre os erros do tipo I e II.
\begin{equation*}
    F\beta = (1+\beta^2)\frac{Precision*Recall}{\beta^2*Precision+ Recall}
\end{equation*}

\subsection{ROC-AUC}
É uma boa escolha quando se procura um limiar específico entre a taxa de verdadeiros positivos e falsos positivos, pois, normalmente, a taxa de verdadeiros positivos e falsos positivos acaba sendo proporcional, quanto mais crescermos a taxa de um, a taxa do outro tende a crescer também.


\section{O Recomendável Para Classes Desbalanceadas}


A acurácia para dados desbalanceados não é provavelmente a melhor opção, visto que nosso modelo está enviesado pela diferença de classes, ele em sua maioria vai acertar grande parte dos casos, mesmo que os erros dfe tipo I e II fiquem mais altos pelo padrão a ser seguido, o seu resultado será alto.

Para um modelo desbalanceado o recomendável é se observar o F1 Score. Já que nosso modelo está inicialmente enviesado para a classe de maior quantidade, a taxa de erros provavelmente será muito alta, fazendo assim com que nosso modelo talvez fique com essa métrica baixíssima.

Quando aplicamo um método para igualar as classes, na acurácia não iremos, provavelmente, perceber uma mudança muito nítida, mas aos compararmos com o F1 Score, iremos verificar, provavlemente, uma mudança para melhor, visto que agora temos um modelo honesto e sem viés, assim provavelmente cometendo menos erros que anteriormente.

Mas, além de se observar isso, é muito necessário, também, observar o ROC-AUC, pois ela acaba sendo em muitas vezes mais precisa no que se procura e também justamente define o limiar para a diferenciação da melhor forma possível entre as duas classes.


\section{Adendo}
O erro do tipo I é a suposição que algo é verdadeiro, mas que na realidade é falso.
O erro do tipo II é a suposição que algo é falso, mas que na realidade é verdadeiro.

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.3\textwidth]{ROC.png}
    \caption{\label{fig:frog}Curva ROC, retirada do código Bank Fraud Logistic Regression}
\end{figure}

\end{document}